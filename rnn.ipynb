{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "50bc16f1018ebb2f7f57adff64b7dfe0f6955bd0b80e5398737879b3684d1ae9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from gensim.models import FastText\r\n",
    "from konlpy.tag import Hannanum\r\n",
    "import sentencepiece as spm\r\n",
    "\r\n",
    "from tqdm import trange\r\n",
    "import os"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\mioeh\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "os.getenv(\"Java_HOME\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'C:\\\\Program Files\\\\Zulu\\\\zulu-8\\\\jre\\\\bin\\\\server'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "dataset_train = []\r\n",
    "dataset_test = []\r\n",
    "dataset_all = []\r\n",
    "\r\n",
    "root = \"newsData/\"\r\n",
    "list = os.listdir(root)\r\n",
    "for cat in list:\r\n",
    "    files = os.listdir(root + cat)\r\n",
    "    for i,f in enumerate(files):\r\n",
    "        fname = root + cat + \"/\" + f\r\n",
    "        file = open(fname, \"r\", encoding=\"utf8\")\r\n",
    "        strings = file.read()\r\n",
    "        if i<170:\r\n",
    "            dataset_train.append([strings, cat])\r\n",
    "        else:\r\n",
    "            dataset_test.append([strings,cat])\r\n",
    "        dataset_all.append(strings)\r\n",
    "        file.close()\r\n",
    "\r\n",
    "print(len(dataset_train), len(dataset_test))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1360 240\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#형태소 단위\r\n",
    "hannanum = Hannanum()\r\n",
    "vocab_morphs = set()\r\n",
    "tokened_morphs = []\r\n",
    "with trange(len(dataset_all)) as tr:\r\n",
    "    for i in tr:\r\n",
    "        morphs = hannanum.morphs(dataset_all[i])\r\n",
    "        for morph in morphs:\r\n",
    "            vocab_morphs.add(morph)\r\n",
    "        tokened_morphs.append(morphs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f = open(\"allsentence.txt\",\"w\")\r\n",
    "f.write(\"\".join(dataset_all).replace(\"\\xa0\", \"\"))\r\n",
    "f.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#subword 단위\r\n",
    "corpus = \"allsentence.txt\"\r\n",
    "prefix = \"news\"\r\n",
    "vocab_size = 8000\r\n",
    "spm.SentencePieceTrainer.train(\r\n",
    "    f\"--input={corpus} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \r\n",
    "    \" --model_type=bpe\" +\r\n",
    "    \" --max_sentence_length=999999\" + # 문장 최대 길이\r\n",
    "    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\r\n",
    "    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\r\n",
    "    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\r\n",
    "    \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\r\n",
    "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vocab_file = \"news.model\"\r\n",
    "vocab = spm.SentencePieceProcessor()\r\n",
    "vocab.load(vocab_file)\r\n",
    "line = \"안녕하세요 만나서 반갑습니다\"\r\n",
    "pieces = vocab.encode_as_pieces(line)\r\n",
    "ids = vocab.encode_as_ids(line)\r\n",
    "print(line)\r\n",
    "print(pieces)\r\n",
    "print(ids)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "안녕하세요 만나서 반갑습니다\n",
      "['▁안', '녕', '하', '세요', '▁만나', '서', '▁반', '갑', '습니다']\n",
      "[89, 7577, 6518, 2892, 957, 6521, 126, 7021, 107]\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# vocab.encode_as_pieces(dataset_all[0])\r\n",
    "tokened_sp = []\r\n",
    "with trange(len(dataset_all)) as tr:\r\n",
    "    for i in tr:\r\n",
    "        tokened_sp.append(vocab.encode_as_pieces(dataset_all[i]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1600/1600 [00:01<00:00, 1228.37it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "emb_num = 128\r\n",
    "\r\n",
    "embedding = FastText(tokened_morphs, vector_size=emb_num, window=12, min_count=5, sg=1)\r\n",
    "embedding.save(\"fasttext_morph.model\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "emb_num = 128\r\n",
    "\r\n",
    "embedding = FastText(tokened_sp, vector_size=emb_num, window=10, min_count=2, sg=1)\r\n",
    "embedding.save(\"fasttext_sp.model\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "emb_num = 128\r\n",
    "model_morphs = FastText.load(\"fasttext_morph.model\")\r\n",
    "model_sp = FastText.load(\"fasttext_sp.model\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_sp.wv.most_similar(\"국회의원\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('▁국회의원', 0.9339150786399841),\n",
       " ('의원', 0.820768415927887),\n",
       " ('▁출마', 0.8091025948524475),\n",
       " ('▁현역', 0.7669360041618347),\n",
       " ('▁지방선거에', 0.762986421585083),\n",
       " ('궐선거', 0.7626964449882507),\n",
       " ('▁사직서', 0.7208353281021118),\n",
       " ('▁송파을', 0.7189249396324158),\n",
       " ('▁사직', 0.7137511372566223),\n",
       " ('▁의원', 0.713568389415741)]"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_morphs.wv.most_similar(\"국회의원\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('의원직', 0.9015116095542908),\n",
       " ('국회의장', 0.8992209434509277),\n",
       " ('사직', 0.8932391405105591),\n",
       " ('출마', 0.8877411484718323),\n",
       " ('사직서', 0.8795743584632874),\n",
       " ('현역의원', 0.8716889023780823),\n",
       " ('사퇴', 0.8710533976554871),\n",
       " ('의원들', 0.8594748973846436),\n",
       " ('보궐선거', 0.8586255311965942),\n",
       " ('현역의원들', 0.8532490134239197)]"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# model_morphs.wv[(hannanum.morphs(\"안녕하세요 감사합니다\"))]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\r\n",
    "from torch import nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.optim as optim\r\n",
    "from torch.utils.data import Dataset, DataLoader\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#using fasttext\r\n",
    "class SentenceDataset(Dataset):\r\n",
    "    def __init__(self, dataset, tokenizer, fasttextModel, max_len):\r\n",
    "        self.sentences = []\r\n",
    "        with trange(len(dataset)) as tr:\r\n",
    "            for i in tr:\r\n",
    "                sen = dataset[i][0]\r\n",
    "                sen = tokenizer(sen)\r\n",
    "                if len(sen) < max_len:\r\n",
    "                    sen = sen + (max_len-len(sen)) * [\"\"]\r\n",
    "                sen = sen[:max_len]\r\n",
    "                sen = fasttextModel[sen]\r\n",
    "                self.sentences.append(sen)\r\n",
    "        self.labels = [np.int32(i[1]) for i in dataset]\r\n",
    "\r\n",
    "    def __getitem__(self, i):\r\n",
    "        return (self.sentences[i],self.labels[i])\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return (len(self.labels))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# using nn.Embedding\r\n",
    "# class SentenceDataset2(Dataset):\r\n",
    "#     def __init__(self, dataset, tokenizer, max_len):\r\n",
    "#         self.sentences = []\r\n",
    "#         with trange(len(dataset)) as tr:\r\n",
    "#             for i in tr:\r\n",
    "#                 sen = dataset[i][0]\r\n",
    "#                 sen = tokenizer(sen)\r\n",
    "#                 if len(sen) < max_len:\r\n",
    "#                     sen = sen + (max_len-len(sen)) * [0]\r\n",
    "#                 sen = sen[:max_len]\r\n",
    "#                 self.sentences.append(sen)\r\n",
    "                \r\n",
    "#         self.sentences = np.array(self.sentences)\r\n",
    "#         self.labels = [np.int32(i[1]) for i in dataset]\r\n",
    "\r\n",
    "#     def __getitem__(self, i):\r\n",
    "#         return (self.sentences[i],self.labels[i])\r\n",
    "\r\n",
    "#     def __len__(self):\r\n",
    "#         return (len(self.labels))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "max_len = 32\r\n",
    "# data_train = SentenceDataset(dataset_train, vocab.encode_as_pieces, model_sp.wv, max_len)\r\n",
    "# data_test = SentenceDataset(dataset_test, vocab.encode_as_pieces,model_sp.wv, max_len)\r\n",
    "\r\n",
    "hannanum = Hannanum()\r\n",
    "data_train = SentenceDataset(dataset_train, hannanum.morphs, model_morphs.wv, max_len)\r\n",
    "data_test = SentenceDataset(dataset_test, hannanum.morphs,model_morphs.wv, max_len)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1360/1360 [01:30<00:00, 14.99it/s]\n",
      "100%|██████████| 240/240 [00:13<00:00, 17.28it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# max_len = 64\r\n",
    "# data_train = SentenceDataset2(dataset_train, vocab.encode_as_ids, max_len)\r\n",
    "# data_test = SentenceDataset2(dataset_test, vocab.encode_as_ids, max_len)\r\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batch_size = 64\r\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5, shuffle=True)\r\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def calc_accuracy(X,Y):\r\n",
    "    max_vals, max_indices = torch.max(X, 1)\r\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\r\n",
    "    return train_acc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class LSTM(nn.Module):\r\n",
    "    def __init__(self, input_size, num_classes, hidden_size, num_layers = 1):\r\n",
    "        super(LSTM, self).__init__()\r\n",
    "        self.input_size = input_size\r\n",
    "        self.hidden_size = hidden_size\r\n",
    "        self.num_layers = num_layers\r\n",
    "        self.num_classes = num_classes\r\n",
    "\r\n",
    "        # self.emb = nn.Embedding(num_embeddings = len(vocab), embedding_dim = 128, padding_idx=0)\r\n",
    "\r\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size,\r\n",
    "                            num_layers = num_layers, batch_first = True)\r\n",
    "\r\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size//2)\r\n",
    "        self.linear2 = nn.Linear(hidden_size//2, num_classes)\r\n",
    "        self.relu = nn.ReLU()\r\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\r\n",
    "    \r\n",
    "        self.fc = nn.Sequential(self.linear, self.dropout, self.relu, self.linear2, self.dropout)\r\n",
    "        # self.fc = nn.Sequential(self.linear, self.dropout)\r\n",
    "\r\n",
    "    def forward(self, x_input):\r\n",
    "        # x_input = self.emb(x_input)\r\n",
    "        lstm_out, (h,c) = self.lstm(x_input)\r\n",
    "        output = self.fc(lstm_out[:,-1,])\r\n",
    "        \r\n",
    "        # hidden = torch.cat((h[-2,:,:], h[-1,:,:]), dim = 1)\r\n",
    "        # output=self.fc(hidden)\r\n",
    "        # output = self.relu(output)\r\n",
    "        # output = self.linear2(output)\r\n",
    "        return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lstm = LSTM(emb_num, 8, 128, 2).to(device)\r\n",
    "\r\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr = 0.0003)\r\n",
    "criterion = nn.CrossEntropyLoss()\r\n",
    "epochs = 100\r\n",
    "\r\n",
    "\r\n",
    "with trange(epochs) as tr:\r\n",
    "    for i in tr:\r\n",
    "        itloss = 0\r\n",
    "        trainacc = 0\r\n",
    "        testacc = 0\r\n",
    "        \r\n",
    "        lstm.train()\r\n",
    "        for batch_id, (input, label) in enumerate(train_dataloader):\r\n",
    "            optimizer.zero_grad()\r\n",
    "            input = input.to(device)\r\n",
    "            label = label.long().to(device)\r\n",
    "            out = lstm(input)\r\n",
    "            loss = criterion(out, label)\r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()\r\n",
    "            itloss += loss.cpu().item()\r\n",
    "            trainacc += calc_accuracy(out,label)\r\n",
    "\r\n",
    "\r\n",
    "        lstm.eval()\r\n",
    "        for batch_idt, (input, label) in enumerate(test_dataloader):\r\n",
    "            input = input.to(device)\r\n",
    "            label = label.long().to(device)\r\n",
    "            out = lstm(input)\r\n",
    "            testacc += calc_accuracy(out,label)\r\n",
    "\r\n",
    "        tr.set_postfix(trainacc=\"{0:.3f}\".format(trainacc/(batch_id+1)), loss=\"{0:.3f}\".format(itloss/(batch_id+1)),  testacc=\"{0:.3f}\".format(testacc/(batch_idt+1)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 100/100 [01:30<00:00,  1.10it/s, loss=0.432, testacc=0.723, trainacc=0.822]\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cate = [\"정치\",\"경제\",\"사회\", \"생활/문화\",\"세계\",\"기술/IT\", \"연예\", \"스포츠\"]\r\n",
    "def softmax(vals, idx):\r\n",
    "    valscpu = vals.cpu().detach().squeeze(0)\r\n",
    "    a = 0\r\n",
    "    for i in valscpu:\r\n",
    "        a += np.exp(i)\r\n",
    "    \r\n",
    "    tmp = []\r\n",
    "    for i in valscpu:\r\n",
    "        tmp.append(((np.exp(i))/a).item() * 100)\r\n",
    "    print([\"{}:{:.2f}%\".format(cate[i],v) for i,v in enumerate(tmp)])\r\n",
    "\r\n",
    "    return ((np.exp(valscpu[idx]))/a).item() * 100\r\n",
    "\r\n",
    "def test_model(seq, model, tokenizer, fasttextmodel):\r\n",
    "    sen = tokenizer(seq)\r\n",
    "    # sen = vocab.encode_as_ids(seq)\r\n",
    "    if len(sen) < max_len:\r\n",
    "        # sen = sen + (max_len-len(sen)) * [1]\r\n",
    "        sen = sen + (max_len-len(sen)) * [\"\"]\r\n",
    "    sen = sen[:max_len]\r\n",
    "    sen = fasttextmodel[sen]\r\n",
    "    sen = torch.tensor(sen).unsqueeze(0).to(device)\r\n",
    "    model.eval()\r\n",
    "    result = model(sen)\r\n",
    "    idx = result.argmax().cpu().item()\r\n",
    "    print(\"뉴스의 카테고리는:\", cate[idx])\r\n",
    "    print(\"신뢰도는:\", \"{:.2f}%\".format(softmax(result,idx)))\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# test_model('신형 아이패드 m1칩 탑재 예정', lstm, vocab.encode_as_pieces, model_sp.wv)\r\n",
    "test_model(\"신형 아이패드 프로에 m1칩 탑재 예정\", lstm, hannanum.morphs, model_morphs.wv)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "뉴스의 카테고리는: 기술/IT\n",
      "['정치:0.00%', '경제:0.16%', '사회:0.15%', '생활/문화:3.64%', '세계:0.12%', '기술/IT:94.37%', '연예:0.13%', '스포츠:1.42%']\n",
      "신뢰도는: 94.37%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 차명종 서대현 김라희 등 韓7명 호치민3쿠션월드컵 PPPQ통과\r\n",
    "# 한국 선수 간 대결이 치러진 A~D조에서는 서대현\r\n",
    "# 대통령, 국회의원 지지율 감소\r\n",
    "# 신형 아이패드 m1칩 탑재 예정"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "torch.save(lstm.state_dict(), \"news_lstm.pt\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\r\n",
    "from konlpy.tag import Hannanum\r\n",
    "\r\n",
    "emb_num=128\r\n",
    "max_len=32\r\n",
    "model_morphs = FastText.load(\"fasttext_morph.model\")\r\n",
    "\r\n",
    "device = torch.device(\"cuda\", index=1)\r\n",
    "modelload = LSTM(emb_num, 8, 128, 2).to(device)\r\n",
    "modelload.load_state_dict(torch.load(\"news_lstm.pt\", device))\r\n",
    "hannanum = Hannanum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_model(\"신형 아이패드 프로에 m1칩 탑재 예정\", modelload, hannanum.morphs, model_morphs.wv)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "뉴스의 카테고리는: 기술/IT\n",
      "['정치:0.00%', '경제:0.16%', '사회:0.15%', '생활/문화:3.64%', '세계:0.12%', '기술/IT:94.37%', '연예:0.13%', '스포츠:1.42%']\n",
      "신뢰도는: 94.37%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}